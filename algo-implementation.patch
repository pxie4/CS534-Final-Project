diff --git a/.vscode/settings.json b/.vscode/settings.json
index 9543f965b7..1c3606f473 100644
--- a/.vscode/settings.json
+++ b/.vscode/settings.json
@@ -3,5 +3,8 @@
         "src/python",
         "ext",
         "tests"
-    ]
+    ],
+    "files.associations": {
+        "*.inc": "cpp"
+    }
 }
diff --git a/src/arch/arm/mmu.cc b/src/arch/arm/mmu.cc
index 47f3a796ca..7a06f4c948 100644
--- a/src/arch/arm/mmu.cc
+++ b/src/arch/arm/mmu.cc
@@ -362,13 +362,66 @@ MMU::translateSe(const RequestPtr &req, ThreadContext *tc, Mode mode,
 
     Process *p = tc->getProcessPtr();
     if (const auto pte = p->pTable->lookup(vaddr); !pte) {
+        DPRINTF(MMU, "TranslateSE failed for %#x\n", vaddr);
         return std::make_shared<GenericPageTableFault>(vaddr_tainted);
     } else {
+
         req->setPaddr(pte->paddr + p->pTable->pageOffset(vaddr));
 
-        if (pte->flags & EmulationPageTable::Uncacheable)
+        if (pte->flags & EmulationPageTable::Uncacheable){
             req->setFlags(Request::UNCACHEABLE);
+        }
 
+        Addr base64 = roundDown(vaddr, 64 * 1024);
+        Addr base16 = roundDown(vaddr, 16 * 1024);
+
+        auto updateStats = [&](Addr base, size_t size, auto &statMap) {
+            auto it = statMap.find(base);
+            // if (it == statMap.end()) {
+            //     DPRINTF(MMU, "No stat entry found for base %#x (size %zu)\n", base, size);
+            //     return;
+            // }
+
+            auto &stats = it->second;
+            stats.totalAccesses++;
+            stats.last_access_tick = curTick();
+            DPRINTF(MMU, "Updating stats for base %#x: totalAccesses = %d\n",
+                    base, stats.totalAccesses);
+
+            uint32_t offset = vaddr - base;
+            DPRINTF(MMU, "Vaddr %#x, base %#x, offset %u\n", vaddr, base, offset);
+
+            if (offset >= size / 2) {
+                stats.upperHalfAccesses++;
+                if (offset >= size * 3 / 4) {
+                    stats.upperUpperQuarterAccesses++;
+                        
+                } else {
+                    stats.upperUpperQuarterAccesses--;
+                }
+            } else {
+                stats.upperHalfAccesses--;
+                if (offset < size / 4) {
+                    stats.lowerLowerQuarterAccesses++;
+                } else {
+                    stats.lowerLowerQuarterAccesses--;
+                }
+            }
+
+            if (stats.totalAccesses == 20) {
+                p->pTable->checkHugePageHotness(vaddr);
+            }
+        };
+        if (p->pTable->page64KBStats.find(base64) != p->pTable->page64KBStats.end()){
+            updateStats(base64, 64 * 1024, p->pTable->page64KBStats);
+        } else {
+            DPRINTF(MMU, "No stat entry found for base %#x in 64KB statmap\n", base64);
+        }
+        if (p->pTable->page16KBStats.find(base16) != p->pTable->page16KBStats.end()){
+            updateStats(base16, 16 * 1024, p->pTable->page16KBStats);
+        } else {
+            DPRINTF(MMU, "No stat entry found for base %#x in 16KB statmap\n", base16);
+        }
         return finalizePhysical(req, tc, mode);
     }
 }
@@ -1203,12 +1256,12 @@ MMU::translateComplete(const RequestPtr &req, ThreadContext *tc,
 {
     bool delay = false;
     Fault fault;
-    if (FullSystem)
+    if (FullSystem){
         fault = translateFs(req, tc, mode, translation, delay, true, tran_type,
             false, state);
-    else
+    }else{
         fault = translateSe(req, tc, mode, translation, delay, true, state);
-
+    }
     DPRINTF(MMU, "Translation returning delay=%d fault=%d\n", delay,
             fault != NoFault);
     // If we have a translation, and we're not in the middle of doing a stage
@@ -1217,7 +1270,20 @@ MMU::translateComplete(const RequestPtr &req, ThreadContext *tc,
     // stage 2 translation we prevent marking the translation as delayed twice,
     // one when the translation starts and again when the stage 1 translation
     // completes.
-
+    // if (fault != NoFault && !FullSystem){
+    //     Addr vaddr_tainted = req->getVaddr();
+    //     Addr vaddr = 0;
+    //     if (state.aarch64) {
+    //         vaddr = purifyTaggedAddr(vaddr_tainted, tc, state.exceptionLevel,
+    //             static_cast<TCR>(state.ttbcr), mode==Execute, state);
+    //     } else {
+    //         vaddr = vaddr_tainted;
+    //     }
+    //     Process *p = tc->getProcessPtr();
+    //     p->fixupFault(vaddr);
+    //     // Cheat way to fix Page Fault (Normal gem5 seems not to be able handle page deallocations)
+    // }
+    
     if (translation && (call_from_s2 || !state.stage2Req || req->hasPaddr() ||
         fault != NoFault)) {
         if (!delay)
diff --git a/src/arch/arm/mmu.hh b/src/arch/arm/mmu.hh
index c6b1ddbb3e..c02cc4d48f 100644
--- a/src/arch/arm/mmu.hh
+++ b/src/arch/arm/mmu.hh
@@ -86,8 +86,11 @@ class MMU : public BaseMMU
     translateFunctional(Addr start, Addr size, ThreadContext *tc,
             Mode mode, Request::Flags flags) override
     {
+        // return TranslationGenPtr(new MMUTranslationGen(
+        //         PageBytes, start, size, tc, this, mode, flags));
+
         return TranslationGenPtr(new MMUTranslationGen(
-                PageBytes, start, size, tc, this, mode, flags));
+                    64*1024, start, size, tc, this, mode, flags));
     }
 
     enum ArmFlags
diff --git a/src/mem/page_table.cc b/src/mem/page_table.cc
index d715e0771f..768e78b3fa 100644
--- a/src/mem/page_table.cc
+++ b/src/mem/page_table.cc
@@ -51,6 +51,17 @@ EmulationPageTable::map(Addr vaddr, Addr paddr, int64_t size, uint64_t flags)
     // starting address must be page aligned
     assert(pageOffset(vaddr) == 0);
 
+    // Detect simulated huge page allocation (64KB chunk of 4KB pages)
+    bool isHugePage = (size == 64 * 1024);
+    if (isHugePage) {
+        Addr base64kAlignedVaddr = roundDown(vaddr, 64 * 1024);
+        DPRINTF(MMU, "Simulating huge page at: %#x (%d KB)\n", vaddr, size / 1024);
+
+        // Track it
+        page64KBStats[base64kAlignedVaddr] = PageAccessStats();
+        page64KBStats[base64kAlignedVaddr].last_access_tick = curTick();
+    }
+
     DPRINTF(MMU, "Allocating Page: %#x-%#x\n", vaddr, vaddr + size);
 
     while (size > 0) {
@@ -132,6 +143,7 @@ EmulationPageTable::isUnmapped(Addr vaddr, int64_t size)
 const EmulationPageTable::Entry *
 EmulationPageTable::lookup(Addr vaddr)
 {
+    
     Addr page_addr = pageAlign(vaddr);
     PTableItr iter = pTable.find(page_addr);
     if (iter == pTable.end())
@@ -139,9 +151,108 @@ EmulationPageTable::lookup(Addr vaddr)
     return &(iter->second);
 }
 
+void
+EmulationPageTable::checkHugePageHotness(Addr vaddr)
+{
+    Addr base64 = roundDown(vaddr, 64 * 1024);
+    Addr base16 = roundDown(vaddr, 16 * 1024);
+
+    DPRINTF(MMU, "Checking huge page hotness for vaddr: 0x%lx\n", vaddr);
+
+    auto checkAndEvict = [&](Addr base, size_t size,
+                             std::unordered_map<Addr, PageAccessStats> &statMap) {
+
+        auto &stats = statMap[base];
+        DPRINTF(MMU, "Stats for base 0x%lx size %lu: upperHalf=%d, upperUpperQuarter=%d, lowerLowerQuarter=%d\n",
+                base, size, stats.upperHalfAccesses, stats.upperUpperQuarterAccesses,
+                stats.lowerLowerQuarterAccesses);
+
+        if (abs(stats.upperHalfAccesses) >= 10 ||
+            abs(stats.upperUpperQuarterAccesses) >= 10 ||
+            abs(stats.lowerLowerQuarterAccesses) >= 10) {
+
+            DPRINTF(MMU, "Eviction/Demotion check triggered for page at base 0x%lx size %lu\n", base, size);
+
+            for (int i = 0; i < size; i += 4 * 1024) {
+                Addr page = base + i;
+                int offset = i;
+
+                if (offset >= size * 3 / 4) {
+                    if ((stats.upperHalfAccesses <= -10) != (stats.upperUpperQuarterAccesses <= -10)) {
+                        DPRINTF(MMU, "Deallocating page 0x%lx in top quarter (3/4 - end)\n", page);
+                        // pTable.erase(page);
+                    } else {
+                        DPRINTF(MMU, "Demoting page 0x%lx from 64kB to 16kB\n", page);
+                        if (size == 64 * 1024) {
+                            Addr new_vaddr = roundDown(page, 16 * 1024);
+                            if (page16KBStats.find(new_vaddr) == page16KBStats.end()) {
+                                DPRINTF(MMU, "Tracking new 16kB page at 0x%lx\n", new_vaddr);
+                                page16KBStats[new_vaddr] = PageAccessStats();
+                            }
+                        }
+                    }
+                } else if (offset >= size * 1 / 2) {
+                    if ((stats.upperHalfAccesses <= -10) != (stats.upperUpperQuarterAccesses >= 10)) {
+                        DPRINTF(MMU, "Deallocating page 0x%lx in upper half (1/2 - 3/4)\n", page);
+                        // pTable.erase(page);
+                    } else {
+                        DPRINTF(MMU, "Demoting page 0x%lx from 64kB to 16kB\n", page);
+                        if (size == 64 * 1024) {
+                            Addr new_vaddr = roundDown(page, 16 * 1024);
+                            if (page16KBStats.find(new_vaddr) == page16KBStats.end()) {
+                                DPRINTF(MMU, "Tracking new 16kB page at 0x%lx\n", new_vaddr);
+                                page16KBStats[new_vaddr] = PageAccessStats();
+                            }
+                        }
+                    }
+                } else if (offset >= size * 1 / 4) {
+                    if ((stats.upperHalfAccesses >= 10) != (stats.lowerLowerQuarterAccesses >= 10)) {
+                        DPRINTF(MMU, "Deallocating page 0x%lx in middle (1/4 - 1/2)\n", page);
+                        // pTable.erase(page);
+                    } else {
+                        DPRINTF(MMU, "Demoting page 0x%lx from 64kB to 16kB\n", page);
+                        if (size == 64 * 1024) {
+                            Addr new_vaddr = roundDown(page, 16 * 1024);
+                            if (page16KBStats.find(new_vaddr) == page16KBStats.end()) {
+                                DPRINTF(MMU, "Tracking new 16kB page at 0x%lx\n", new_vaddr);
+                                page16KBStats[new_vaddr] = PageAccessStats();
+                            }
+                        }
+                    }
+                } else {
+                    if ((stats.upperHalfAccesses >= 10) != (stats.lowerLowerQuarterAccesses <= -10)) {
+                        DPRINTF(MMU, "Deallocating page 0x%lx in lower quarter (0 - 1/4)\n", page);
+                        // pTable.erase(page);
+                    } else {
+                        DPRINTF(MMU, "Demoting page 0x%lx from 64kB to 16kB\n", page);
+                        if (size == 64 * 1024) {
+                            Addr new_vaddr = roundDown(page, 16 * 1024);
+                            if (page16KBStats.find(new_vaddr) == page16KBStats.end()) {
+                                DPRINTF(MMU, "Tracking new 16kB page at 0x%lx\n", new_vaddr);
+                                page16KBStats[new_vaddr] = PageAccessStats();
+                            }
+                        }
+                    }
+                }
+            }
+
+            DPRINTF(MMU, "Removing stats for base 0x%lx from statMap\n", base);
+            statMap.erase(base);
+        } else {
+            stats.totalAccesses = 0;
+        }
+    };
+
+    checkAndEvict(base64, 64 * 1024, page64KBStats);
+    checkAndEvict(base16, 16 * 1024, page16KBStats);
+}
+
+       
+
 bool
 EmulationPageTable::translate(Addr vaddr, Addr &paddr)
 {
+    DPRINTF(MMU, "1 translate ");
     const Entry *entry = lookup(vaddr);
     if (!entry) {
         DPRINTF(MMU, "Couldn't Translate: %#x\n", vaddr);
@@ -156,6 +267,7 @@ Fault
 EmulationPageTable::translate(const RequestPtr &req)
 {
     Addr paddr;
+    DPRINTF(MMU, "2 translate ");
     assert(pageAlign(req->getVaddr() + req->getSize() - 1) ==
            pageAlign(req->getVaddr()));
     if (!translate(req->getVaddr(), paddr))
@@ -165,6 +277,7 @@ EmulationPageTable::translate(const RequestPtr &req)
         panic("Request spans page boundaries!\n");
         return NoFault;
     }
+
     return NoFault;
 }
 
@@ -172,6 +285,7 @@ void
 EmulationPageTable::PageTableTranslationGen::translate(Range &range) const
 {
     const Addr page_size = pt->pageSize();
+    
 
     Addr next = roundUp(range.vaddr, page_size);
     if (next == range.vaddr)
diff --git a/src/mem/page_table.hh b/src/mem/page_table.hh
index a17250c17d..83e6dc4bcb 100644
--- a/src/mem/page_table.hh
+++ b/src/mem/page_table.hh
@@ -62,11 +62,24 @@ class EmulationPageTable : public Serializable
         Entry() {}
     };
 
+    struct PageAccessStats {
+      uint64_t totalAccesses = 0;
+      Tick last_access_tick = 0;
+      uint32_t upperHalfAccesses = 0;
+      uint32_t upperUpperQuarterAccesses = 0;
+      uint32_t lowerLowerQuarterAccesses = 0;
+    };
+    std::unordered_map<Addr, Entry> pTable;
+    // MY CHANGES
+    std::unordered_map<Addr, PageAccessStats> page64KBStats;
+    std::unordered_map<Addr, PageAccessStats> page16KBStats;
+    uint64_t global_translation_counter; // used to periodiaclly scan for cold pages 
+    //---------------------
   protected:
     typedef std::unordered_map<Addr, Entry> PTable;
     typedef PTable::iterator PTableItr;
-    PTable pTable;
-
+    
+   
     const Addr _pageSize;
     const Addr offsetMask;
 
@@ -142,6 +155,12 @@ class EmulationPageTable : public Serializable
     const Entry *lookup(Addr vaddr);
 
     /**
+     * New added function
+     */
+    void checkHugePageHotness(Addr vaddr);
+
+    /**
+     * 
      * Translate function
      * @param vaddr The virtual address.
      * @param paddr Physical address from translation.
diff --git a/src/mem/se_translating_port_proxy.cc b/src/mem/se_translating_port_proxy.cc
index 8f4544c80f..4d354f8275 100644
--- a/src/mem/se_translating_port_proxy.cc
+++ b/src/mem/se_translating_port_proxy.cc
@@ -56,13 +56,31 @@ SETranslatingPortProxy::fixupRange(const TranslationGen::Range &range,
         BaseMMU::Mode mode) const
 {
     auto *process = _tc->getProcessPtr();
-
     if (mode == BaseMMU::Write) {
+        // if (allocating == Always) {
+        //     constexpr int64_t hugePageSize = 64 * 1024;
+        //     // const Addr start = roundDown(range.vaddr, hugePageSize);
+        //     const Addr end = range.vaddr + range.size;
+
+        //     for (Addr addr = range.vaddr; addr < end; addr += hugePageSize) {
+        //         // Allocate only the remaining size if we're at the final chunk.
+        //         int64_t allocSize = std::min((Addr)(end - addr), (Addr)hugePageSize);
+        //         process->allocateMem(addr, allocSize);
+        //     }
+
+        //     return true;
+        // } else if (allocating == NextPage &&
+        //     process->fixupFault(range.vaddr)) {
+        //     return true;
+        // }
+
         if (allocating == Always) {
+            // DPRINTF(Vma, "Range with allocateMem with size: %#x,", range.size);
             process->allocateMem(range.vaddr, range.size);
             return true;
         } else if (allocating == NextPage &&
                 process->fixupFault(range.vaddr)) {
+                    // DPRINTF(Vma, "Range with fixupFault with size: %#x,", range.size);
             // We've accessed the next page on the stack.
             return true;
         }
diff --git a/src/sim/mem_state.cc b/src/sim/mem_state.cc
index 93e7ca3773..9059f5fc76 100644
--- a/src/sim/mem_state.cc
+++ b/src/sim/mem_state.cc
@@ -395,10 +395,31 @@ MemState::fixupFault(Addr vaddr)
      * Check if we are accessing a mapped virtual address. If so then we
      * just haven't allocated it a physical page yet and can do so here.
      */
+    DPRINTF(Vma, "Is fixupFault() called");
     for (const auto &vma : _vmaList) {
         if (vma.contains(vaddr)) {
             Addr vpage_start = roundDown(vaddr, _pageBytes);
-            _ownerProcess->allocateMem(vpage_start, _pageBytes);
+            Addr huge_page_base = roundDown(vaddr, 64 * 1024);
+            // Check if *any* 4KB pages inside the 64KB range are already mapped
+            bool subpagesExist = false;
+            for (int offset = 0; offset < 64 * 1024; offset += 4 * 1024) {
+                Addr page = huge_page_base + offset;
+                if (_ownerProcess->pTable->pTable.find(page) != _ownerProcess->pTable->pTable.end()) {
+                    subpagesExist = true;
+                    break;
+                }
+            }
+
+            // Only allocate huge page if none of the 4KB pages are mapped
+            if (!subpagesExist) {
+                DPRINTF(Vma, "Allocating 64KB huge page at virtual address range: [0x%lx-0x%lx)\n",
+                    huge_page_base, huge_page_base + 64 * 1024);
+                _ownerProcess->allocateMem(huge_page_base, 64 * 1024);
+            } else {
+                DPRINTF(Vma, "Allocating 4KB page at virtual address range: [0x%lx-0x%lx)\n",
+                    vpage_start, vpage_start + _pageBytes);
+                _ownerProcess->allocateMem(vpage_start, _pageBytes); // fallback to 4KB
+            }
 
             /**
              * We are assuming that fresh pages are zero-filled, so there is
@@ -430,6 +451,8 @@ MemState::fixupFault(Addr vaddr)
      * yet.
      */
     if (vaddr >= _stackMin && vaddr < _stackBase) {
+        DPRINTF(Vma, "Allocating 4KB stack page at address range: [0x%lx-0x%lx)\n",
+            vaddr, vaddr + _pageBytes);
         _ownerProcess->allocateMem(roundDown(vaddr, _pageBytes), _pageBytes);
         return true;
     }
@@ -444,12 +467,39 @@ MemState::fixupFault(Addr vaddr)
             if (_stackBase - _stackMin > _maxStackSize) {
                 fatal("Maximum stack size exceeded\n");
             }
+            DPRINTF(Vma, "Growing stack by one 4KB page: new stackMin=0x%lx\n", _stackMin);
+        
             _ownerProcess->allocateMem(_stackMin, _pageBytes);
             inform("Increasing stack size by one page.");
         }
         return true;
     }
 
+    // // Making Changes Trying to Default Page FixUP regardless of VMA
+    // Addr vpage_start = roundDown(vaddr, _pageBytes);
+    // Addr huge_page_base = roundDown(vaddr, 64 * 1024);
+    // // Check if *any* 4KB pages inside the 64KB range are already mapped
+    // bool subpagesExist = false;
+    // for (int offset = 0; offset < 64 * 1024; offset += 4 * 1024) {
+    //     Addr page = huge_page_base + offset;
+    //     if (_ownerProcess->pTable->pTable.find(page) != _ownerProcess->pTable->pTable.end()) {
+    //         subpagesExist = true;
+    //         break;
+    //     }
+    // }
+
+    // // Only allocate huge page if none of the 4KB pages are mapped
+    // if (!subpagesExist) {
+    //     DPRINTF(Vma, "Allocating 64KB huge page at virtual address range: [0x%lx-0x%lx)\n",
+    //         huge_page_base, huge_page_base + 64 * 1024);
+    //     _ownerProcess->allocateMem(huge_page_base, 64 * 1024);
+    //     return true;
+    // } else {
+    //     DPRINTF(Vma, "Allocating 4KB page at virtual address range: [0x%lx-0x%lx)\n",
+    //         vpage_start, vpage_start + _pageBytes);
+    //     _ownerProcess->allocateMem(vpage_start, _pageBytes); // fallback to 4KB
+    //     return true;
+    // }
     return false;
 }
 
diff --git a/src/sim/process.cc b/src/sim/process.cc
index 7b6e2d0de2..db299a2ebe 100644
--- a/src/sim/process.cc
+++ b/src/sim/process.cc
@@ -318,7 +318,8 @@ Process::allocateMem(Addr vaddr, int64_t size, bool clobber)
 {
     const auto page_size = pTable->pageSize();
 
-    const Addr page_addr = roundDown(vaddr, page_size);
+    Addr align = (size == 64 * 1024) ? 64 * 1024 : page_size;
+    const Addr page_addr = roundDown(vaddr, align);
 
     // Check if the page has been mapped by other cores if not to clobber.
     // When running multithreaded programs in SE-mode with DerivO3CPU model,
@@ -341,6 +342,8 @@ Process::allocateMem(Addr vaddr, int64_t size, bool clobber)
     pTable->map(page_addr, paddr, pages_size,
                 clobber ? EmulationPageTable::Clobber :
                           EmulationPageTable::MappingFlags(0));
+    DPRINTF(Vma, "Mapped virtual addr %#x to physical addr %#x, size: %#x, clobber: %d\n",
+                            page_addr, paddr, pages_size, clobber);
 }
 
 void
